% 
% Annual Cognitive Science Conference
% Sample LaTeX Paper -- Proceedings Format
% 

% Original : Ashwin Ram (ashwin@cc.gatech.edu)       04/01/1994
% Modified : Johanna Moore (jmoore@cs.pitt.edu)      03/17/1995
% Modified : David Noelle (noelle@ucsd.edu)          03/15/1996
% Modified : Pat Langley (langley@cs.stanford.edu)   01/26/1997
% Latex2e corrections by Ramin Charles Nakisa        01/28/1997 
% Modified : Tina Eliassi-Rad (eliassi@cs.wisc.edu)  01/31/1998
% Modified : Trisha Yannuzzi (trisha@ircs.upenn.edu) 12/28/1999 (in process)
% Modified : Mary Ellen Foster (M.E.Foster@ed.ac.uk) 12/11/2000
% Modified : Ken Forbus                              01/23/2004
% Modified : Eli M. Silk (esilk@pitt.edu)            05/24/2005
% Modified : Niels Taatgen (taatgen@cmu.edu)         10/24/2006
% Modified : David Noelle (dnoelle@ucmerced.edu)     11/19/2014
% Modified : Roger Levy (rplevy@mit.edu)             12/31/2018



%% Change "letterpaper" in the following line to "a4paper" if you must.

\documentclass[10pt,letterpaper]{article}

\usepackage{cogsci}

\cogscifinalcopy % Uncomment this line for the final submission 


\usepackage{pslatex}
\usepackage{apacite}
\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{float} % Roger Levy added this and changed figure/table
                   % placement to [H] for conformity to Word template,
                   % though floating tables and figures to top is
				   % still generally recommended!
\usepackage{amsmath}

%\usepackage[none]{hyphenat} % Sometimes it can be useful to turn off
%hyphenation for purposes such as spell checking of the resulting
%PDF.  Uncomment this block to turn off hyphenation.

\definecolor{Orange}{RGB}{255,140,0}
\newcommand{\ek}[1]{\textcolor{Orange}{[ek: #1]}} 

\definecolor{Purple}{RGB}{255,10,140}
\newcommand{\jd}[1]{\textcolor{Purple}{[jd: #1]}} 

\newcommand{\tableref}[1]{Table~\ref{#1}}
\newcommand{\figref}[1]{Fig.~\ref{#1}}


\setlength\titlebox{4.5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 4.5cm (the original size).
%%If you do, we reserve the right to require you to change it back in
%%the camera-ready version, which could interfere with the timely
%%appearance of your paper in the Proceedings.



\title{Production Expectations Modulate Contrastive Inference}
 
\author{{\large \bf Elisa Kreiss (ekreiss@stanford.edu)} \\
 Department of Linguistics, 460 Jane Stanford Way \\
 Stanford, CA 94305 USA
 \AND {\large \bf Judith Degen (jdegen@stanford.edu)} \\
 Department of Linguistics, 460 Jane Stanford Way \\
 Stanford, CA 94305 USA}


\begin{document}

\maketitle

\begin{abstract}
Contrastive inferences, whereby a listener pragmatically infers a speaker's referential intention of a partial referring expression like \emph{the yellow} by reasoning about other objects in the context, are notoriously unstable. We report a production-centric model of interpretation couched within the Rational Speech Act framework. Adjective production probabilities a listener expects for objects in a context drive the size of contrastive inferences: the greater the asymmetry in expectation for a speaker to use a pre-nominal adjective for the target rather than for competitors, the greater the listener's resulting target preference. Modifier production probabilities were collected (Exp.~1) and used to make predictions about comprehension in an incremental decision task (Exp.~2). The model's interpretation predictions are supported by the data. This account has the potential to explain the fluctuating appearance of contrastive inferences and shifts the explanatory focus away from contrastive inference towards online interpretation of referring expressions more broadly.

\textbf{Keywords:} 
contrastive inference; RSA; typicality; incremental processing
\end{abstract}

\section{Introduction}

One of the most interesting features of language is its flexibility. In referring to an object, speakers choose from a wealth of possible referring expressions. \textit{The banana}, \textit{the yellow banana}, and \textit{the curvy fruit} are all expressions that can refer to the same object. Moreover, the same utterance -- e.g., \textit{the banana} -- can be used to refer to different kinds of objects (yellow bananas, brown bananas, etc.). This flexibility poses a challenge for listeners, who have been shown to rapidly draw pragmatic inferences about speakers' referential intentions in online processing. Consequently, understanding how listeners process referring expressions -- in particular, to what extent contextual information enters into this process -- has been a central topic of psycholinguistic research.

\begin{figure}
	\begin{center}
		\includegraphics[width=.475\textwidth]{graphs/example-context.pdf}
	\end{center}
\caption{Three contexts, each with a yellow banana as the target and another yellow object as its competitor. The competitor can be typical (A, B) or atypical (C), and a contrast can be absent (A) or present (B, C). Gray stars represent other distractors that do not share color or type with any other object.} 
\label{example-context}
\end{figure}

Language is processed incrementally. Eye-tracking experiments have shown that upon hearing an incomplete utterance like \textit{the yellow} in a display like \figref{example-context}a, listeners start to fixate the yellow objects more than other objects even before they hear the disambiguating noun \textit{banana} \cite{Eberhard:1995}. Additionally listeners go beyond the information contained in the signal itself in processing language; they also take into account contextual information -- including the nature of other possible referents -- to draw rapid pragmatic inferences about a speaker's intended referent. One such inference that has received much attention in recent years is the so-called \emph{contrastive inference} \cite{Sedivy:1999,Aparicio:2018,Grodner:2011,Rubio-Fernandez:2019,Ryskin:2019}. Consider the context in \figref{example-context}b that shows a yellow and an orange banana, a yellow corncob and some other distractor item. When a listener is asked to \textit{Click on the yellow...}, there are two eligible objects to choose from: the yellow banana and the yellow corncob. Rather than considering both yellow objects equally likely target referents, listeners often exhibit a preference, evidenced by increased looks, for the yellow object that has a contrasting member of the same type and different color in the display \cite<i.e., the banana,>{Sedivy:1999, Sedivy:2003}. When the contrast is absent, as in \figref{example-context}a, listeners have no such preference. This preference for the target over the competitor elicited by the presence of a contrast (i.e., the orange banana) is considered the result of drawing a contrastive inference. 

Contrastive inferences arise as the result of listeners expecting a cooperative speaker to not be more informative than required by the context \cite{Grice:1975}. The presence of a contrast object makes it contextually necessary to include the adjective. In contrast, the adjective is not necessary to refer to the competitor object. Upon observing the adjective, listeners can reverse-engineer that the intended referent must be the color-congruent object with a contrast member, i.e., the yellow banana in \figref{example-context}b \cite{Aparicio:2018,Grodner:2011,Ryskin:2019,Sedivy:1999}.

This simple Gricean account that only takes into consideration the contrastive function of the adjective predicts that contrastive inferences should arise whenever the target object occurs in the presence of a contrast object. It is surprising, then, that contrastive inferences are not consistently observed across experiments. While the contrastive inference effect has been replicated reliably in the size adjective domain \cite{Aparicio:2018,Grodner:2011,Heller:2008,Ryskin:2019,Sedivy:1999}, the effect is less stable with color adjectives \cite{Sedivy:2003}. \citeA{Sedivy:2003} reports that the contrastive inference arises in contexts where the target object has a predictable color (such as the yellow banana in \figref{example-context}) but not when it is replaced by an object with an unpredictable color like a cup, which comes in many colors.
She shows that these objects differ in how likely a speaker is to produce the color modifier for the object in isolation: in the absence of a contrast, a yellow banana is usually called \textit{the banana} while a yellow cup is often called \textit{the yellow cup}, which \citeA{Sedivy:2003} calls these objects' \emph{default descriptions}. Only in cases where the modifier is not part of the default description, she argues, is its observation surprising and can be interpreted as a signal that will elicit the contrastive inference.

In addition to expectations of informativity as described above, contrastive inferences have been proposed to depend on the semantics of the adjective involved, such that it reliably arises with relative adjectives (e.g., size adjectives) and maximum standard absolute adjectives (e.g., \textit{full}), but not with minimum standard absolute adjectives (e.g., \textit{empty}); while the evidence from color adjectives is conflicting \cite{Rubio-Fernandez:2019,Sedivy:2003}. Furthermore the effect disappears when the listener considers the speaker unreliable \cite{Grodner:2011,Ryskin:2019}.

In this paper, we investigate an account of contrastive inference that has the potential to unify the above properties by reducing them to listeners' expectations about the speaker's contextual probability of producing the pre-nominal adjective. In so doing, we follow recent research highlighting the importance of listeners' generative model of the speaker in generating pragmatic inferences \cite{Hawkins:2018,Kao:2015,Kleinschmidt:2011,Macdonald:1994,Mitchell:1995,Rubio-Fernandez:2018}. We formalize the relevant listener-side reasoning within the Rational Speech Act (RSA) framework \cite{Frank:2012, Goodman:2016, Cohn-Gordon:2018}, a state-of-the-art computational framework that models pragmatic inference as the result of listeners performing Bayesian inference on their speaker model and their prior beliefs about likely meanings, thereby giving the speaker model a central role in the inference. It provides a way to quantitatively assess the probabilities that a listener assigns to possible referents after observing partial sentences of the form \emph{Click on the yellow\dots} given their prior beliefs and expectations about the speaker. This account shifts the explanatory focus away from specific cognitive and linguistic factors that influence contrastive inference and towards listener's production expectations (and their prior beliefs, which we don't treat in depth in this paper).

For this investigation it is important to distinguish between two notions: the behavioral pattern that manifests as a \emph{target preference}, i.e., a preference for the target over the competitor; and the theoretical construct of a \emph{contrastive inference}, i.e., the increase in target preference when a contrast is present vs.~when it is absent.

We proceed by first showing that our production-centric account makes the same qualitative predictions about the basic contrastive inference effect as for instance the default description account proposed by \citeA{Sedivy:2003}. We then derive new predictions about the size of target preferences across different contrast-present and contrast-absent contexts. We report a free production study (Exp.~1) we conducted to elicit modifier probability estimates, which we used to determine quantitative model predictions. We evaluate the model by comparing those predictions to empirical comprehension data which we elicited using an incremental decision task (Exp.~2).


\section{A Bayesian account of contrastive inference}

The Rational Speech Act framework \cite{Frank:2012,Goodman:2016} is a probabilistic (and thus non-deterministic) Bayesian account of natural language which ascribes a central role to the speaker in pragmatic interpretation. The core idea of the model is that a listener and a speaker recursively reason about each other: A pragmatic listener $L_1$ wants to infer the meaning of an utterance $u$, as formulated by the pragmatic speaker $S_1$. Possible referents $r$ are assigned a probability proportional to the probability that $S_1$ will produce $u$ to convey $r$ multiplied by the listener's prior belief in $r$ $P(r)$, as defined by Bayes' Rule.\footnote{The pragmatic speaker model and further recursive steps are spelled out in detail elsewhere \cite<e.g.,>{Goodman:2016}. Since we will elicit speaker probabilities empirically, we need not be concerned with the details of the speaker model.}

\begin{equation}
	P_{L_1}(r|u) \propto P_{S_1}(u|r) * P(r)
\label{eq-prior}
\end{equation}

To simplify the following example, we will assume that listeners have a uniform prior $P(r)$ over all objects in the display\footnote{The simplifying assumption is justified by the results of Exp.~2.}. Then the RSA model predicts a direct relationship between the production probabilities $P_{S_1}$ and the listener's distribution over possible referents $P_{L_1}$.

While RSA has typically been applied to the analysis of full utterances, it can straightforwardly be extended to generate predictions at the sub-sentential level\footnote{One exception is the Incremental Iterated Response Model of Pragmatics, which is also shown to qualitatively predict contrastive inference in general \cite{Cohn-Gordon:2018}.}. To generate RSA predictions for an incomplete referring expression such as \textit{Click on the yellow...}, we take $P_{S_1}$ to correspond to the contextual probability of color mention for each referent in the display. This corresponds to marginalizing over the probabilities of all continuations of the utterance (i.e., \textit{Click on the yellow banana/corncob/lettuce/...!}). Let's investigate this account's qualitative predictions:

Consider the example contexts in \figref{example-context}a and \ref{example-context}b. Upon hearing the modifier \textit{yellow}, the pragmatic listener $P_{L_1}$ considers how likely a speaker is to include this modifier in their referring expression for each object in the display. Since only the target (yellow banana) and the competitor (corncob) are yellow, we assume that the production probabilities of \textit{yellow} for the other objects in the display are 0. This only leaves the target and the competitor as potential referents. 

Hypothetical modifier production probabilities for target and competitor are shown in the middle row of \figref{example-context}. 
Assume that in the absence of a contrast object (\figref{example-context}a), speakers are equally unlikely to include the color modifier when referring to the target banana (probability 0.1) and its color competitor, the corncob (0.1). Pragmatic listener predictions are obtained by renormalizing these probabilities, resulting in a target preference of 0.5, i.e., the pragmatic listener does not prefer one potential referent over the other.

Does RSA predict the target preference and therefore contrastive inference in context \figref{example-context}b? Assuming that the presence of the contrasting orange banana does not affect the speaker's modifier production probability for the competitor corncob but does increase modifier production probability for the target banana to 0.9, renormalizing the production probabilities results in a target preference of 0.9 -- thus reproducing the classic contrastive inference.

Unlike previous accounts of contrastive inference, modifier production probabilities are expected to directly drive the contrastive inference and associated target preference. Since the contrastive inference is the difference in target preference between contrast conditions and the target preference depends on the modifier production probabilities of the target and the competitor, the competitor takes on a central role in these predictions. This suggests that increasing the modifier production probabilities for the competitor should lead to a decrease in target preference. It has been established that speakers are more likely to include color modifiers in referring expressions for objects in isolation when they appear in an atypical rather than in a typical color \cite{Rubio-Fernandez:2016,Westerbeek:2015,Degen:2020}. Thus the atypical yellow strawberry in \figref{example-context}c is more likely to elicit a color modifier than the typical corncob in \figref{example-context}b. Assuming a modifier production probability of 0.6, this contrast-present context yields a much smaller increase in target preference compared to the contrast-absent context. In other words, the size of the target preference is predicted to be dependent on the choice of competitor in the contrast-present vs.~contrast-absent conditions, keeping target typicality constant. This predicts that the size of the contrastive inference can vary depending not only on features of the target \cite<as previously shown by>{Sedivy:2003, Rubio-Fernandez:2019}, but also crucially depending on features of the competitor (and more generally, any other objects in the display that may plausibly elicit the relevant modifier).

% \jd{these last few paragraphs that bring in typicality and go through predictions are very nicely done}

To investigate this novel prediction, we first elicited modifier production probabilities (i.e., an estimate of $P_{S_1}(u|r)$) in a free production interactive reference game (Exp.~1) in contexts that varied in the presence of a contrast, the typicality of the target, and the typicality of the competitor. This allowed us to generate pragmatic listener probabilities for each display. We then evaluated model performance by comparing these predictions to empirically elicited interpretations (Exp.~2).


\section{Experiment 1: Modifier Production in an Interactive Reference Game}

The goal of Exp.~1 was to obtain color modifier production probabilities for the items in the displays ultimately used in the contrastive inference experiment (Exp.~2). In particular, we elicited production probabilities for those items that functioned as targets and competitors in Exp.~2.\footnote{We assumed that the production probability of the relevant color modifier was close to 0 for the remaining distractor objects in the display and did not elicit these explicitly.}  Probabilities were elicited in a free production interactive reference game. We expected modifier production probability to be higher for atypical objects and in the presence of a contrast. For instance, we expected speakers to call a yellow banana simply \textit{the banana}, but an orange banana \textit{the orange banana}. We employed the elicited modifier production probabilities as the pragmatic speaker probabilities in the subsequent model evaluation.

\subsection{Method}

%\noindent \textbf{Participants.} We recruited 282 participants over Amazon's Mechanical Turk, who were randomly matched to form director-matcher dyads (i.e., 141 pairs in total).%Each participant was paid \$2.30 (approximately \$11-\$14/hr)
% \footnote{The experiment was preregistered on \texttt{https://osf.io/57h9n}.}
% Originally, we recruited 68 participants and then ran a follow-up with 214 more to get enough data for the evaluation of the RSA model. The results from the first 68 participants do not differ from the full data set, which is why we present them collapsed.}. We restricted participation to workers with US-based IP addresses and an approval rate of at least 97\%.


% Exclusions were performed on the 141 speakers, since they provided the utterances. Participants were excluded when they participated multiple times in the experiment (1 participant; 139 pairs remaining) and when they did not use a noun from the display in at least half of the cases (27 participants; 112 pairs remaining). These participants clearly misunderstood the task, using expressions such as \textit{yellow monkey} instead of \textit{yellow banana}, or \textit{should be yellow, must have teeth to eat} for \textit{corn}. All speakers indicated that their native language was English.


We recruited 282 participants over Amazon's Mechanical Turk, who were randomly matched to form director-matcher dyads (i.e., 141 pairs in total). 

Each context included four objects, as displayed in \figref{example-context}. The pool of objects consisted of 10 items (e.g., broccoli), each of which could occur in a typical (green broccoli) and atypical color (red broccoli). All objects were carefully normed for color-diagnosticity \cite{Tanaka:1999}, typicality, and nameability. Both director and matcher saw the same four objects, but in scrambled positions. The director also saw a green border around one object which was to be described to the matcher through a chat window. The matcher's task was to click on this object. 

% Each context included four items, as displayed in \figref{prod-design}. The pool of items consisted of 10 types (banana, broccoli, carrot, corn, egg, lettuce, pumpkin, strawberry, swan, tomato), each of which could occur in a typical and atypical color. For example, the broccoli could occur in its typical color green or in the atypical color red. The resulting pool contained 20 items, 10 of which were atypically colored. The number of colors were counterbalanced such that each color occurred twice as a typical instance and twice as an atypical one. All items were carefully normed for color-diagnosticity \cite{Tanaka:1999}, typicality and nameability.

On critical trials, participants saw critical displays from Exp.~2. The object to be communicated could be either the object that functioned as the target or the object that functioned as the competitor in that display in Exp.~2, as exemplified in \figref{example-context}. We continue to refer to `target' and `competitor' in the reporting of this experiment, terms which refer to the function of the object to be communicated in Exp.~2. Contexts varied in the typicality of the target and the competitor and the presence of a contrast, resulting in eight conditions. Participants saw each context exactly once. Throughout the experiment, half of the critical trials required the speaker to communicate the `target' and in the other half the `competitor'.

In contexts where the contrast was absent, the distinction between target and competitor was meaningless and thus one of the color competitor objects was arbitrarily coded as the target and the other as the competitor. Fillers were randomly created contexts where the `contrast' or the `distractor' from \figref{example-context} was the object to be communicated. Overall, each dyad saw 60 contexts (32 critical trials) in randomized order.

% The contexts varied in the typicality of the target, the typicality of the competitor and the presence of a contrast, resulting in eight conditions. 
% We needed to elicit the modifier production probabilities for the target and the competitor. In contexts where the contrast was absent, this distinction was irrelevant. For example, when the target and competitor are both (a)typical, either could be underlyingly coded as the target. Similarly, the modifier production probability for a typical target with an atypical competitor is the same as the probability for a typical competitor in a context with an atypical target. In contrast-present contexts, the target-competitor distinction matters, which is why speakers had to communicate the competitor half of the trials and the target in the other half.
% Fillers were eight randomly created contexts where the contrast or the distractor had to be communicated.
% Overall, each participant saw 60 different contexts (32 critical trials) in a completely randomized order.

%
%\subsection{Procedure}
%The speaker-listener pairs could communicate freely through a real-time multi-player interface similar to \cite{Hawkins:2015}. The speaker was instructed to communicate a target object out of a four-object context to the listener. The target could be identified by a green border surrounding it. The speaker and the listener saw the same set of objects but in a randomized order to avoid trivial position-based references such as ``the left one''. After the listener clicked on the presumed target, both the speaker and listener received feedback about whether the right object had been selected.
%
%\begin{figure}
%	\begin{center}
%		\includegraphics[width=.475\textwidth]{graphs/prod-design.pdf}
%	\end{center}
%\caption{Example display for the interactive reference game (Exp.~1). Both, the speaker (here \textit{Director}) and listener (\textit{Matcher}) see the same four objects but in a scrambled order. Additionally, the speaker sees a green border around one of the objects, marking the intended target which the listener needs to select.} 
%\label{prod-design}
%\end{figure}


\subsection{Results}
We excluded two dyads because of multiple participation and 27 dyads for primarily using playful descriptions, e.g., \textit{should be yellow, must have teeth to eat} for the \textit{red corn} object, which left 112 dyads for the analysis.

\figref{prod-results} shows the proportion of color modifier mentions for the target and competitor in each condition. We conducted a Bayesian mixed effects logistic regression predicting color mention for each item from centered fixed effects of contrast presence, target typicality, and competitor typicality, as well as random by-participant intercepts (the most complex random effects structure that allowed the model to converge). 

There was strong evidence of contrast presence ($E=5.25,\ CI=[4.82, 5.69]$), such that when a contrast to the object was present (e.g., another banana, see target proportions in the upper row in \figref{prod-results}), participants were more likely to mention the color modifier than in the absence of a contrast (see target proportions in the lower row in \figref{prod-results} and competitor proportions overall). This was especially true when the object was atypical\footnote{A full interaction model did not converge because color was \emph{always} mentioned in the contrast-present condition with atypical targets, which did not allow the model to generate estimates for interactions involving these conditions. We did not find evidence for any other interactions.}. There was also strong evidence for the object's typicality ($E=2.82,\ CI=[2.52, 3.12]$), such that participants were more likely to include a color modifier when referring to an atypical object than a typical one.

\begin{figure}
	\begin{center}
		\includegraphics[width=.475\textwidth]{graphs/prod-bycond-paper.pdf}
	\end{center}
\caption{Proportion of modifier mentions in each condition for objects that functioned as target and competitor in Exp.~2. Error bars indicate 95\% bootstrapped confidence intervals.} 
\label{prod-results}
\end{figure}

The results of this production experiment show that the probability of a speaker's modifier use is modulated by an object's color typicality, replicating previous results \cite{Westerbeek:2015}. The results also confirm the assumption made by many contrastive inference studies that speakers are more likely to produce the color modifier in the presence of a contrast \cite{Aparicio:2018,Grodner:2011,Sedivy:1999}, though this probability is modulated by the typicality of the object.
% Our experiment therefore successfully manipulates the modifier production probabilities a listener can expect in different contexts.


\section{Experiment 2: Referential Interpretation in an Incremental Decision Task}

To investigate which object listeners consider to be the most likely referent after observing the color adjective, we conducted an incremental decision task \cite{Qing:2018}. This is an offline task that allows for eliciting participants' belief distributions at multiple points in the unfolding referring expression. 

\subsection{Method}

% \footnote{The experiment was preregistered on \texttt{https://osf.io/27dn8}.}% Originally, we recruited 80 participants and then ran a follow-up with 140 more to get enough data for the evaluation of the RSA model. The results from the first 80 participants do not differ from the full data set, which is why we present them collapsed.}. Each of them were paid \$1.80 for their participation (10\$-16\$/hr). The same restrictions for participation applied as in Exp.~1. 


% We recruited 239 participants over Amazon's Mechanical Turk, 121 of which saw atypical color competitors and 118 saw typical color competitors in the critical trials\footnote{The experiment was preregistered on \texttt{https://osf.io/27dn8}. Originally, we recruited 80 participants and then ran a follow-up with 140 more to get enough data for the evaluation of the RSA model. The results from the first 80 participants do not differ from the full data set, which is why we present them collapsed.}. Each of them were paid \$1.80 for their participation (10\$-16\$/hr). We restricted participation to workers with IP addresses in the US and an approval rate of previous work above 97\%. 
% 27 participants were excluded because they indicated that they did the experiment incorrectly, English was not their native language, or they gave more than 20\% erroneous responses\footnote{An erroneous response is defined as a selection of a non-target object after observing the fully disambiguating noun.}. 211 participants remain, 108 of which were in the atypical competitor and 103 were in the typical competitor condition. 

% We excluded participants who did the Hit multiple times (1), who indicated that they did the Hit incorrectly or were confused (13), who indicated that they had a native language other than English (6), and who gave more then 20\% erroneous responses (7). An erroneous response is defined as a click to a non-target object after observing the fully disambiguating noun, i.e., participants are excluded who selected the wrong final object more than 11 times. Overall, we excluded 27 people, which is 11\% of the subjects. 211 participants remain, 108 of which were in the atypical competitor and 103 were in the typical competitor condition. 

We recruited 239 participants over Amazon's Mechanical Turk. This experiment was a one-player comprehension-only adaptation of the production study described above and was implemented as an incremental decision task \cite{Qing:2018}: Participants read sentences of the form ``Click on the yellow banana'', which contained a referring expression, and their task was to select the target in the display. Crucially, the sentence was only gradually revealed. Participants made a selection at each of three time points: (1) before receiving any information about the referent (i.e, after observing ``Click on the'', \emph{prior window}), (2) after observing the adjective (``Click on the yellow''), \emph{adjective window}, and (3) after observing the full referring expression with the disambiguating noun (``Click on the yellow banana''), \emph{noun window}. 

The critical displays were identical to the critical displays in Exp.~1. Target typicality and contrast presence were within-participant manipulations, competitor typicality was a between-participants manipulation\footnote{The complexity of the 2x2x2 design and considerations of power required that either the number of trials per participant  be high or one manipulation  be between-participants. We decided for a smaller number of trials to minimize the probability of strategic responses or response fatigue developing over the course of the experiment. Contrast presence and target typicality could not be manipulated between-participants since these regularities are easily detectable by a participant within an experiment. Between-participants manipulations are considered more conservative \cite{Charness:2012} and random by-participant intercepts and slopes were included in the analyses to account for random by-participant variability.}. All critical trials used color modified referring expressions. 
Filler trials were included that primarily used unmodified utterances and referred to one of the other three items in the display to avoid learning effects. 
Participants completed 55 trials (20 critical) in random order. To minimize the risk that the speaker was perceived as pragmatically uncooperative \cite{Grodner:2011,Pogue:2016,Ryskin:2019}, trials with modified utterances that referred to a typical object with no contrast only appeared after the 15th trial. To familiarize participants with the task, they first completed four practice trials in the director role.
% Participants completed 55 trials (20 critical) that were randomized with the only restriction that modified utterances that referred to a typical object with no contrast only appeared after the 15th trial to minimize the risk that the speaker was perceived as pragmatically uncooperative \cite{Grodner:2011}. To familiarize participants with the task, they first completed four practice from the director's perspective.
 
 
%To center the position of the mouse after each selection, a button appeared in the center of the grid which had to be clicked to reveal the next word or to advance to the next trial. 


% Participants completed 55 trials in total, 20 of which were critical trials and 35 were fillers. The contexts varied for each participant with respect to the presence of a contrast and the target's color typicality (within-subject manipulation). Participants were randomly assigned to see either typical or atypical competitors on critical trials (between-subject manipulation). All critical trials included color modified utterances. 
% To avoid learning effects, we included filler trials with unmodified referring expressions and with referents other than the targets of the critical trials. 


% \begin{figure}
% 	\begin{center}
% 		\includegraphics[width=.325\textwidth]{graphs/IDT-design.pdf}
% 	\end{center}
% \caption{Design of the incremental decision task. The referring expression was placed above the grid and revealed gradually. After each new word participants made a selection indicating their best guess about the intended target.} 
% \label{IDT-design}
% \end{figure}


% This experiment is a one-player adaptation of the production study explained above and follows the design of an incremental decision task \cite{Qing:2018}. 
% All participant were assigned the role of the listener, which means that they needed to identify which object was the target given a referring expression placed above the context. Crucially, the referring expression was only gradually revealed and participants had to choose an object each time before the trial continued. In each critical trial, three choices had to be made: (1) before receiving any information about the referent (i.e, after observing ``Click on the''), (2) after receiving the adjective (``Click on the yellow'') and (3) after receiving the full referring expression with the disambiguating noun (``Click on the yellow banana!''). 
% To center the position of the mouse after each selection, a button appeared in the center of the grid which had to be clicked to reveal the next word or to advance to the next trial. 
% Trials were randomized with the only restriction that modified utterances that referred to a typical object with no contrast only appeared after the 15th trial to minimize the risk that the speaker was perceived as unreliable \cite{Grodner:2011}.
% Before participants proceeded to the main trials, they had to complete four practice trials constructed from the speaker perspective, which were introduced to familiarize the participants with the task.

\subsection{Results}
We excluded participants who participated multiple times (1), who indicated that they did the experiment incorrectly or were confused (13), whose self-reported native language was not English (6), and who gave more than 20\% erroneous responses\footnote{An incorrect response is defined as a selection of a non-target object after observing the fully disambiguating noun.} (7).
211 participants remained; 108 saw atypical competitors and 103 saw typical competitors on critical trials. 

\figref{modelcompr-results} shows the proportion of target and competitor selections in the adjective window (lighter colors) alongside the RSA model predictions derived from the Exp.~1 production probabilities (darker colors), grouped by condition.\footnote{Neither of the other two objects in the display was chosen after observing the adjective.} %First, we focus on the empirical results where we predicted that the presence of the contrast and the typicality of the objects will affect the listeners' object choices. 
We conducted a Bayesian mixed effects logistic regression on adjective window choices, predicting the log odds of target over competitor selections from centered fixed effects of contrast presence, target typicality, competitor typicality, and their interactions, prior window selection, as well as the maximal random effects structure that allowed the model to converge\footnote{Random effects: $(1+\text{contrast}*\text{target\_typicality}|\text{participant}) + (1+\text{contrast}*\text{competitor\_typicality}|\text{target}) + (1+\text{contrast}*\text{target\_typicality}|\text{competitor})$}.

There was strong evidence for an effect of contrast presence ($E=0.34,\ CI=[0.13,0.53]$), such that when there was a contrast object (top panels), there was a general preference for target over competitor selections, replicating the standard contrastive inference effect. This preference was largest when the target was atypical and the competitor was typical and disappeared when the target was typical and the competitor was atypical, following the qualitative predictions discussed in the modeling section above and exemplified in \figref{example-context}. There was also strong evidence for an effect of competitor typicality ($E=-0.54,\ CI=[-0.90,-0.17]$), such that when the competitor was atypical, target selections decreased, which is again in line with our predictions.

% When there was no contrast (bottom panels in \figref{modelcompr-results}) and target and competitor differed in typicality, there was a preference for the object with the atypical color. When the two objects were similarly (a)typical, participants showed no preference for one over the other, again in line with the qualitative predictions.

% \jd{again, no more mixed effects analysis at all? it's really weird to say "there was/wasn't a difference" and not back it up with any stats whatsoever}

% Before an adjective is observed, all items should appear equally likely to be the target, which is supported by the generally uniform distribution in all conditions. After the adjective is revealed (darker colors), only the target and competitor are legible options and we predicted that the presence of the contrast and the typicality of the objects will affect the listeners' object choices.

% \begin{figure}
% 	\begin{center}
% 		\includegraphics[width=.475\textwidth]{graphs/compr-results.pdf}
% 	\end{center}
% \caption{Results for the comprehension study, showing the proportion of selections for each item in the display and each condition. The bars in lighter colors indicate the selections before, the darker bars are the selections after the adjective was observed. Error bars are 95\% bootstrapped confidence intervals.} 
% \label{compr-results}
% \end{figure}

\begin{figure}
	\begin{center}
		\includegraphics[width=.475\textwidth]{graphs/model-bycond-paper.pdf}
	\end{center}
\caption{Empirical proportion (light bars) and model predicted probability (in darker colors) of object selections for each condition. Dashed line marks chance level of target versus competitor selections. Error bars indicate 95\% bootstrapped confidence intervals.} %for each target-competitor item combination.} 
\label{modelcompr-results}
\end{figure}

Although object selections in the prior window were approximately at chance, there was strong evidence that it affected participants' specific selections of their adjective window choices ($E=1.46,\ CI=[1.29,1.63]$). These results suggest that when participants' prior selection is congruent with the newly revealed adjectival information, they stick with their previous choice. 
% This observation will be relevant for later discussions of the size of target preference.
% However the main effects of contrast ($E=0.34,\ CI=[0.14,0.53]$) and competitor typicality ($E=-0.54,\ CI=[-0.90,-0.17]$) still remain.

% Before the adjective was observed, object selections were approximately at chance. However, the selections after reading the adjective were affected by the participant's previous selection, such that a participant who previously selected the competitor was more likely to select the competitor again than switch to the target (and vice versa). But since object selections before the adjective followed a uniform distribution, any patterns that appear after the adjective cannot be an artifact of the reselection bias.

Overall, these results suggest that the color typicality of not just the target, but of competitor objects in the display, too, affects the inferences listeners draw about the intended referent. An atypical competitor alone can promote the competitor over the target when the contrast is absent and can even make the target preference disappear when a contrast is present. %It is therefore highly relevant to control for the quality of the competitor when assessing contrastive inferences. 

If one quantifies contrastive inference as an increased target preference in the adjective window in the contrast-present condition compared to its item-matched contrast-absent condition, the contrastive inferences is small or even non-existent when the target is atypical and the competitor typical (left column of \figref{modelcompr-results}). This may explain why contrastive inferences did not occur with target items of unpredictable colors \cite{Sedivy:2003}. However, even though those items have been reported to have a higher modifier production probability in isolation \cite{Sedivy:2003}, future work still needs to establish how those objects of unpredictable colors relate to (a)typically colored objects.

\section{Model evaluation}

Here we assess the extent to which RSA captures the comprehension data based on the empirically elicited modifier production probabilities. We assume a flat prior over all objects in the display, a choice justified by the uniform selection distribution over objects in the prior window of the comprehension experiment. The pragmatic listener probabilities assigned to the target over the competitor are then the normalized modifier production probabilities as shown in Equation~(\ref{eq-flatprior}), where $r$ are contextually possible referents $r_{\text{target}}$ and $r_{\text{comp}}$, and $u$ the referring expression up to the contextually warranted color modifier, e.g., \emph{the yellow}.

\begin{equation}
	P_{L_1}(r|u) = \frac{P_{S_1}(u|r)}{P_{S_1}(u|r_{\text{target}}) + P_{S_1}(u|r_{\text{comp}})}
\label{eq-flatprior}
\end{equation}

\figref{modelcompr-results} shows the model predictions (dark bars) alongside the empirical results (light bars) for target and competitor selection in the adjective window. Using the modifier production probabilities obtained in Exp.~1, the model predicts the qualitative patterns for all the different context conditions. 

Quantitatively we found strong evidence that the RSA model predicts the empirically elicited comprehension data ($E=1.46, CI=[1.01, 1.92]$)\footnote{Results of a Bayesian mixed effects logistic regression model: $\text{target\_selection} \sim \text{RSA\_prediction} + (1+\text{RSA\_prediction}|\text{participant})$} and its predictions are highly correlated with the empirical results ($r=0.91$). However, it generally predicts more extreme probabilities than are borne out in the empirical data, as shown in \figref{model-results-corr-flatprior}. The model overpredicts target selections in high target preference conditions and underpredicts target selections in low target preference conditions. 

\begin{figure}
	\begin{center}
		\includegraphics[width=.475\textwidth]{graphs/corr-plot.pdf}
	\end{center}
\caption{Empirical proportion of target selections against RSA model predictions. The dashed lines mark chance level for target over competitor selections. Error bars indicate 95\% bootstrapped confidence intervals.} 
\label{model-results-corr-flatprior}
\end{figure}

One possible explanation for the mismatch between model predictions and observed target selections towards the extreme ends of the scale is that the empirically elicited contrastive inferences appear smaller due to participants' re-selection bias (as described in the results of Exp.~2). If a participant observes an adjective that could elicit a contrastive inference but the participant selected the competitor in the prior window, the re-selection bias counteracts the contrastive inference. This can explain why the range of empirical target selection proportions is compressed towards the center of the scale. We have since re-run this experiment without eliciting prior window selections, and the results support this explanation. In an eye-tracking version of the experiment, where participants do not make explicit looking decisions, the bias to continue looking at the same object may also be weaker.

% One possible explanation is that the inferences appear smaller in the comprehension study because of the bias to reselect the previously selected object (as described in the results of Exp.~2). If a participant observes an adjective that could elicit a contrastive inference but the participant selected the competitor in the prior window, the non-switching bias counteracts the contrastive inference. This can explain why the range of empirical target selection proportions is compressed towards chance. Whether this is indeed an artifact of the incremental decision task could be investigated by re-running the experiment without a prior window selection. Similarly, in an eye-tracking version of the experiment, where participants do not make explicit looking decisions, the non-switching bias is likely to be weaker.

Overall, these results suggest a strong connection between referring expression interpretation and production. Only using the probability of encountering the observed adjective, the RSA model can qualitatively and quantitatively predict the empirically elicited comprehension data. 
% Although we replicate that the contrastive inference can be elicited in an offline incremental decision task, the model results suggest that the selection biases in the paradigm might reduce the size of the inferences. We expect this bias to reduce in an eye-tracking paradigm, which is an immediate future direction for this work.


\section{General Discussion}

In this paper, we tested a speaker-centric model of contrastive inference couched within the Rational Speech Act (RSA) framework. We used the model to make quantitative predictions about the behavior a pragmatic listener should exhibit in varying contexts. In contrast to previous accounts, it is not simply the modifier production probability for the target that modulates the inference \cite<as suggested by, e.g., the default description account proposed by>{Sedivy:2003}, but more broadly the relative modifier production probabilities for \emph{all} contextually relevant objects. This account shifts the focus away from specific cognitive and linguistic factors that have been discussed to affect contrastive inference and onto listener's production expectations, and away from contrastive inference narrowly to the interpretation of referring expressions more broadly. 

We show that this speaker-centric model not only predicts the basic contrastive inference effect; it also provides possible explanations for why contrastive inferences are less stable with color adjectives. First, the nature of the competitor affects the behavioral patterns generally associated with contrastive inference, such that the target preference can disappear even when a contrast is present, as long as the expected modifier production probabilities are sufficiently similar for target and competitor. Second, higher modifier production probabilities for the target in contrast-absent contexts can decrease the difference in target preference compared to its otherwise matched contrast-present context. 

These results also provide a challenge for accounts that would explain away variable contrastive inference behavior by pointing towards adjective semantics. The presented comprehension results show a high degree of variability in target preference within the color adjective domain, calling into question a generalizable contrastive inference pattern for color adjectives. 
While an adjective semantics-based account predicts greater variability between than within adjective types, a speaker-centric account predicts instead that differences in target preference and contrastive inference are mediated by a listener's expectations about how likely the modifier is to be produced. 
% Our results further challenge `default description' accounts that are solely based on the identity of the noun that the adjective modifies, since target preference is not only affected by the presence of the contrast but crucially also by properties of the target and the competitor. 

The strong correlation between the model predictions and target preference patterns in the comprehension experiment suggests a clear connection between production expectations and pragmatic interpretation. To derive listener predictions, the presented model used empirically elicited modifier production probabilities, essentially treating the speaker model as a black box. One avenue of future work is to apply existing RSA models of modified referring expression production \cite{Degen:2020}, which have been successful in modeling the empirically observed redundant use of color modifiers as a function of objects' typicality, to the production data reported here.

Finally, the RSA model predicts that a listener's prior beliefs about likely referents should affect listeners' inferences in a systematic way. In other words, a listener's target preference should be greater for objects they believe the speaker is more likely to refer to a priori. Explicit prior manipulations, and extensions of the model to other adjectives are promising new avenues to further probe the RSA account of the interpretation of referring expressions. 



































% In contrast to previous accounts, it is not simply the production probability of the target that modulates the inference \cite<as suggested by the default description account>{Sedivy:2003}, but more broadly the relative modifier production probability of \emph{all} contextually relevant objects. %This means that it assigns a central role to the color competitor in the display. 
% The empirical results confirm that the choice of the competitor affects the interpretation of the utterance, providing evidence for this highly pragmatic account of comprehension.
% We assessed the model predictions by collecting object selections in an incremental decision task \cite{Qing:2018}, replicating that this paradigm is generally suited for eliciting contrastive inferences. Crucially, the results show high variation between context conditions dependent on the typicality of the target and competitor. In other words, by varying the modifier production probabilities, we can make the contrastive inference appear strong and almost make it disappear. These results provide a challenge for accounts that would explain away variable contrastive inference behavior by pointing towards adjective semantics. A speaker-centric account predicts instead that the general differences observed between different types of adjectives is in fact mediated by how likely a listener expects the modifier to be produced. 

% and an interesting new avenue for future investigations. \jd{this discussion is still kind of mixed up. parts of the last paragraph are redundant with the first paragraph of the discussion. also: important to mention the explicit prior manipulation as an avenue for a stronger test of the rsa model. another test for the model: extending to other adjectives }
















% \ek{random thought: if target is typical and contrast present -- does the typicality of the contrast affect modifier production for the target? It should. So far we see that modifier mention for the typical target + contrast contexts is not at ceiling, because (maybe) "banana" is still a better description of the target than the contrast. This would probably change if the contrast was brown.}

% You can't take the results wrt presence/size of the contrastive inference effect and draw conclusions about the nature of adjective types. We can take color adjectives and make the contrastive inference (dis-)appear, i.e., we can make them look like size or material adjectives.

% We are showing that
% 1) listeners are highly pragmatic, i.e., they consider how well the utterance fits to each possible referent in the context and draws inferences about each and then takes them together -> competitor matters
% 2) thereby the modifier does not simply seem to receive a contrastive explanation for mentioning, but also a typicality explanation which affects the final inference just as much
% 3) taken together there simply seems to be an expectation of modifier inclusion which can result from multiple rationales.
% 4) we can make the contrastive inference strong (as for size adjectives) or disappear
% 5) in RSA we don't need to assume anything about the nature of the adjective, but can simply consider what the speaker probabilities for modifier use are... i.e., the linguistic knowledge can be obscure: we just need to consider production probabilities to predict listener behavior; underlyingly this could be simply communicative pressures; Due to informativity, the color needs to be mentioned if there is a contrast and the same holds for typicality (see PsychReview paper)

% \ek{thought: the term contrastive inference is so confusing. Because there is a contrastive inference in the tan-tap case, but it's not simply: "Is there a target preference in the contrast present case", but is there a target preference compared to the non-contrast condition. I.e., if you define the contrastive inference as the preference of the target over the competitor when the contrast is present, there is no contrastive inference in the tap condition. But if you define it as the change in preference from contrast absent to contrast present, then there still is a contrastive inference. This issue makes describing the "occurrence of contrastive inference" difficult here -- maybe we should address that?}




























% \section{General Formatting Instructions}

% The entire content of a paper (including figures, references, and anything else) can be no longer than six pages in the \textbf{initial submission}. In the \textbf{final submission}, the text of the paper, including an author line, must fit on six pages. Up to one additional page can be used for acknowledgements and references.

% The text of the paper should be formatted in two columns with an
% overall width of 7 inches (17.8 cm) and length of 9.25 inches (23.5
% cm), with 0.25 inches between the columns. Leave two line spaces
% between the last author listed and the text of the paper; the text of
% the paper (starting with the abstract) should begin no less than 2.75 inches below the top of the
% page. The left margin should be 0.75 inches and the top margin should
% be 1 inch. \textbf{The right and bottom margins will depend on
%  whether you use U.S. letter or A4 paper, so you must be sure to
%  measure the width of the printed text.} Use 10~point Times Roman
% with 12~point vertical spacing, unless otherwise specified.

% The title should be in 14~point bold font, centered. The title should
% be formatted with initial caps (the first letter of content words
% capitalized and the rest lower case). In the initial submission, the
% phrase ``Anonymous CogSci submission'' should appear below the title,
% centered, in 11~point bold font. In the final submission, each
% author's name should appear on a separate line, 11~point bold, and
% centered, with the author's email address in parentheses. Under each
% author's name list the author's affiliation and postal address in
% ordinary 10~point type.

% Indent the first line of each paragraph by 1/8~inch (except for the
% first paragraph of a new section). Do not add extra vertical space
% between paragraphs.


% \section{First Level Headings}

% First level headings should be in 12~point, initial caps, bold and
% centered. Leave one line space above the heading and 1/4~line space
% below the heading.


% \subsection{Second Level Headings}

% Second level headings should be 11~point, initial caps, bold, and
% flush left. Leave one line space above the heading and 1/4~line
% space below the heading.


% \subsubsection{Third Level Headings}

% Third level headings should be 10~point, initial caps, bold, and flush
% left. Leave one line space above the heading, but no space after the
% heading.


% \section{Formalities, Footnotes, and Floats}

% Use standard APA citation format. Citations within the text should
% include the author's last name and year. If the authors' names are
% included in the sentence, place only the year in parentheses, as in
% \citeA{NewellSimon1972a}, but otherwise place the entire reference in
% parentheses with the authors and year separated by a comma
% \cite{NewellSimon1972a}. List multiple references alphabetically and
% separate them by semicolons
% \cite{ChalnickBillman1988a,NewellSimon1972a}. Use the
% ``et~al.'' construction only after listing all the authors to a
% publication in an earlier reference and for citations with four or
% more authors.


% \subsection{Footnotes}

% Indicate footnotes with a number\footnote{Sample of the first
% footnote.} in the text. Place the footnotes in 9~point font at the
% bottom of the column on which they appear. Precede the footnote block
% with a horizontal rule.\footnote{Sample of the second footnote.}


% \subsection{Tables}

% Number tables consecutively. Place the table number and title (in
% 10~point) above the table with one line space above the caption and
% one line space below it, as in Table~\ref{sample-table}. You may float
% tables to the top or bottom of a column, and you may set wide tables across
% both columns.

% \begin{table}[H]
% \begin{center} 
% \caption{Sample table title.} 
% \label{sample-table} 
% \vskip 0.12in
% \begin{tabular}{ll} 
% \hline
% Error type   & Example \\
% \hline
% Take smaller     &  63 - 44 = 21 \\
% Always borrow~~~~  &  96 - 42 = 34 \\
% 0 - N = N       &  70 - 47 = 37 \\
% 0 - N = 0       &  70 - 47 = 30 \\
% \hline
% \end{tabular} 
% \end{center} 
% \end{table}


% \subsection{Figures}

% All artwork must be very dark for purposes of reproduction and should
% not be hand drawn. Number figures sequentially, placing the figure
% number and caption, in 10~point, after the figure with one line space
% above the caption and one line space below it, as in
% \figref{sample-figure}. If necessary, leave extra white space at
% the bottom of the page to avoid splitting the figure and figure
% caption. You may float figures to the top or bottom of a column, and
% you may set wide figures across both columns.

% \begin{figure}[H]
% \begin{center}
% \fbox{CoGNiTiVe ScIeNcE}
% \end{center}
% \caption{This is a figure.} 
% \label{sample-figure}
% \end{figure}


% \section{Acknowledgments}

% In the \textbf{initial submission}, please \textbf{do not include
%  acknowledgements}, to preserve anonymity. In the \textbf{final submission},
% place acknowledgments (including funding information) in a section \textbf{at
% the end of the paper}.


% \section{References Instructions}

% Follow the APA Publication Manual for citation format, both within the
% text and in the reference list, with the following exceptions: (a) do
% not cite the page numbers of any book, including chapters in edited
% volumes; (b) use the same format for unpublished references as for
% published ones. Alphabetize references by the surnames of the authors,
% with single author entries preceding multiple author entries. Order
% references by the same authors by the year of publication, with the
% earliest first.

% Use a first level section heading, ``{\bf References}'', as shown
% below. Use a hanging indent style, with the first line of the
% reference flush against the left margin and subsequent lines indented
% by 1/8~inch. Below are example references for a conference paper, book
% chapter, journal article, dissertation, book, technical report, and
% edited volume, respectively.

% \nocite{ChalnickBillman1988a}
% \nocite{Feigenbaum1963a}
% \nocite{Hill1983a}
% \nocite{OhlssonLangley1985a}
% % \nocite{Lewis1978a}
% \nocite{Matlock2001}
% \nocite{NewellSimon1972a}
% \nocite{ShragerLangley1990a}


\bibliographystyle{apacite}

\setlength{\bibleftmargin}{.125in}
\setlength{\bibindent}{-\bibleftmargin}

\bibliography{CogSci_Template}


\end{document}
